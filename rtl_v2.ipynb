{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from mltu.configs import BaseModelConfigs\n",
    "\n",
    "class ModelConfigs(BaseModelConfigs):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model_path = os.path.join(\"Models/04_sentence_recognition\", datetime.strftime(datetime.now(), \"%Y%m%d%H%M\"))\n",
    "        self.vocab = \"\"\n",
    "        self.height = 96\n",
    "        self.width = 1408\n",
    "        self.max_text_length = 0\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.0005\n",
    "        self.train_epochs = 1000\n",
    "        self.train_workers = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interface model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Models/04_sentence_recognition/202301131202/configs.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmltu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModelConfigs\n\u001b[1;32m---> 30\u001b[0m configs \u001b[38;5;241m=\u001b[39m \u001b[43mBaseModelConfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModels/04_sentence_recognition/202301131202/configs.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m ImageToWordModel(model_path\u001b[38;5;241m=\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mmodel_path, char_list\u001b[38;5;241m=\u001b[39mconfigs\u001b[38;5;241m.\u001b[39mvocab)\n\u001b[0;32m     34\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModels/04_sentence_recognition/202301131202/val.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\Kingstone\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mltu\\configs.py:35\u001b[0m, in \u001b[0;36mBaseModelConfigs.load\u001b[1;34m(configs_path)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(configs_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfigs_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     36\u001b[0m         configs \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[0;32m     38\u001b[0m     config \u001b[38;5;241m=\u001b[39m BaseModelConfigs()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Models/04_sentence_recognition/202301131202/configs.yaml'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "from mltu.inferenceModel import OnnxInferenceModel\n",
    "from mltu.utils.text_utils import ctc_decoder, get_cer, get_wer\n",
    "from mltu.transformers import ImageResizer\n",
    "\n",
    "class ImageToWordModel(OnnxInferenceModel):\n",
    "    def __init__(self, char_list: typing.Union[str, list], *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.char_list = char_list\n",
    "\n",
    "    def predict(self, image: np.ndarray):\n",
    "        image = ImageResizer.resize_maintaining_aspect_ratio(image, *self.input_shapes[0][1:3][::-1])\n",
    "\n",
    "        image_pred = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "\n",
    "        preds = self.model.run(self.output_names, {self.input_names[0]: image_pred})[0]\n",
    "\n",
    "        text = ctc_decoder(preds, self.char_list)[0]\n",
    "\n",
    "        return text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    from tqdm import tqdm\n",
    "    from mltu.configs import BaseModelConfigs\n",
    "\n",
    "    configs = BaseModelConfigs.load(\"Models/04_sentence_recognition/202301131202/configs.yaml\")\n",
    "\n",
    "    model = ImageToWordModel(model_path=configs.model_path, char_list=configs.vocab)\n",
    "\n",
    "    df = pd.read_csv(\"Models/04_sentence_recognition/202301131202/val.csv\").values.tolist()\n",
    "\n",
    "    accum_cer, accum_wer = [], []\n",
    "    for image_path, label in tqdm(df):\n",
    "        image = cv2.imread(image_path.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "        prediction_text = model.predict(image)\n",
    "\n",
    "        cer = get_cer(prediction_text, label)\n",
    "        wer = get_wer(prediction_text, label)\n",
    "        print(\"Image: \", image_path)\n",
    "        print(\"Label:\", label)\n",
    "        print(\"Prediction: \", prediction_text)\n",
    "        print(f\"CER: {cer}; WER: {wer}\")\n",
    "\n",
    "        accum_cer.append(cer)\n",
    "        accum_wer.append(wer)\n",
    "\n",
    "        cv2.imshow(prediction_text, image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    print(f\"Average CER: {np.average(accum_cer)}, Average WER: {np.average(accum_wer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.models import Model\n",
    "\n",
    "from mltu.tensorflow.model_utils import residual_block\n",
    "\n",
    "\n",
    "def train_model(input_dim, output_dim, activation=\"leaky_relu\", dropout=0.2):\n",
    "    \n",
    "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
    "\n",
    "    # normalize images here instead in preprocessing step\n",
    "    input = layers.Lambda(lambda x: x / 255)(inputs)\n",
    "\n",
    "    x1 = residual_block(input, 32, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "\n",
    "    x2 = residual_block(x1, 32, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x3 = residual_block(x2, 32, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    x4 = residual_block(x3, 64, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x5 = residual_block(x4, 64, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    x6 = residual_block(x5, 128, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x7 = residual_block(x6, 128, activation=activation, skip_conv=True, strides=1, dropout=dropout)\n",
    "\n",
    "    x8 = residual_block(x7, 128, activation=activation, skip_conv=True, strides=2, dropout=dropout)\n",
    "    x9 = residual_block(x8, 128, activation=activation, skip_conv=False, strides=1, dropout=dropout)\n",
    "\n",
    "    squeezed = layers.Reshape((x9.shape[-3] * x9.shape[-2], x9.shape[-1]))(x9)\n",
    "\n",
    "    blstm = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(squeezed)\n",
    "    blstm = layers.Dropout(dropout)(blstm)\n",
    "\n",
    "    blstm = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(blstm)\n",
    "    blstm = layers.Dropout(dropout)(blstm)\n",
    "\n",
    "    output = layers.Dense(output_dim + 1, activation=\"softmax\", name=\"output\")(blstm)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmltu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model2onnx, TrainLogger\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmltu\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CERMetric, WERMetric\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfigs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelConfigs\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Replace IAM dataset path with your own folder\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from mltu.preprocessors import ImageReader\n",
    "from mltu.transformers import ImageResizer, LabelIndexer, LabelPadding, ImageShowCV2\n",
    "from mltu.augmentors import RandomBrightness, RandomRotate, RandomErodeDilate, RandomSharpen\n",
    "from mltu.annotations.images import CVImage\n",
    "from mltu.tensorflow.dataProvider import DataProvider\n",
    "from mltu.tensorflow.losses import CTCloss\n",
    "from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n",
    "from mltu.tensorflow.metrics import CERMetric, WERMetric\n",
    "from model import train_model\n",
    "from configs import ModelConfigs\n",
    "\n",
    "# Replace IAM dataset path with your own folder\n",
    "input_folder_path = r'C:\\Users\\Kingstone\\Desktop\\All folder\\project work\\IDRBT_Cheque_Image_Dataset\\IDRBT Cheque Image Dataset\\300\\input_folder'  # Set your folder path here\n",
    "label_file_path = 'path_to_labels.txt'  # Optional: Path to text file with labels\n",
    "\n",
    "# Initialize variables to store dataset and vocabulary\n",
    "dataset, vocab, max_len = [], set(), 0\n",
    "\n",
    "# If there's a label file, load labels from it (assuming format: \"image_filename label\")\n",
    "if os.path.exists(label_file_path):\n",
    "    with open(label_file_path, \"r\") as label_file:\n",
    "        labels = {line.split()[0]: ' '.join(line.split()[1:]).replace(\"|\", \" \").strip() for line in label_file.readlines()}\n",
    "else:\n",
    "    labels = {}\n",
    "\n",
    "# Iterate over images in the folder\n",
    "for root, dirs, files in os.walk(input_folder_path):\n",
    "    for file_name in tqdm(files):\n",
    "        if file_name.endswith((\".png\", \".jpg\", \".jpeg\" , \".tif\" , \".tiff\")):  # Process only image files\n",
    "            image_path = os.path.join(root, file_name)\n",
    "            \n",
    "            # If labels are provided, use them; otherwise, use blank label\n",
    "            label = labels.get(file_name, \"\")  \n",
    "            \n",
    "            # Add image and label to dataset\n",
    "            dataset.append([image_path, label])\n",
    "            vocab.update(list(label))\n",
    "            max_len = max(max_len, len(label))\n",
    "\n",
    "# Create a ModelConfigs object to store model configurations\n",
    "configs = ModelConfigs()\n",
    "\n",
    "# Save vocab and maximum text length to configs\n",
    "configs.vocab = \"\".join(vocab)\n",
    "configs.max_text_length = max_len\n",
    "configs.save()\n",
    "\n",
    "# Create a data provider for the dataset\n",
    "data_provider = DataProvider(\n",
    "    dataset=dataset,\n",
    "    skip_validation=True,\n",
    "    batch_size=configs.batch_size,\n",
    "    data_preprocessors=[ImageReader(CVImage)],\n",
    "    transformers=[\n",
    "        ImageResizer(configs.width, configs.height, keep_aspect_ratio=True),\n",
    "        LabelIndexer(configs.vocab),\n",
    "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data_provider, val_data_provider = data_provider.split(split=0.9)\n",
    "\n",
    "# Augment training data with random brightness, rotation, and erode/dilate\n",
    "train_data_provider.augmentors = [\n",
    "    RandomBrightness(),\n",
    "    RandomErodeDilate(),\n",
    "    RandomSharpen(),\n",
    "]\n",
    "\n",
    "# Creating TensorFlow model architecture\n",
    "model = train_model(\n",
    "    input_dim=(configs.height, configs.width, 3),\n",
    "    output_dim=len(configs.vocab),\n",
    ")\n",
    "\n",
    "# Compile the model and print summary\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate),\n",
    "    loss=CTCloss(),\n",
    "    metrics=[\n",
    "        CERMetric(vocabulary=configs.vocab),\n",
    "        WERMetric(vocabulary=configs.vocab)\n",
    "    ],\n",
    "    run_eagerly=False\n",
    ")\n",
    "model.summary(line_length=110)\n",
    "\n",
    "# Define callbacks\n",
    "earlystopper = EarlyStopping(monitor=\"val_CER\", patience=20, verbose=1, mode=\"min\")\n",
    "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.h5\", monitor=\"val_CER\", verbose=1, save_best_only=True, mode=\"min\")\n",
    "trainLogger = TrainLogger(configs.model_path)\n",
    "tb_callback = TensorBoard(f\"{configs.model_path}/logs\", update_freq=1)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor=\"val_CER\", factor=0.9, min_delta=1e-10, patience=5, verbose=1, mode=\"auto\")\n",
    "model2onnx = Model2onnx(f\"{configs.model_path}/model.h5\")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_data_provider,\n",
    "    validation_data=val_data_provider,\n",
    "    epochs=configs.train_epochs,\n",
    "    callbacks=[earlystopper, checkpoint, trainLogger, reduceLROnPlat, tb_callback, model2onnx],\n",
    "    workers=configs.train_workers\n",
    ")\n",
    "\n",
    "# Save training and validation datasets as CSV files\n",
    "train_data_provider.to_csv(os.path.join(configs.model_path, \"train.csv\"))\n",
    "val_data_provider.to_csv(os.path.join(configs.model_path, \"val.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
